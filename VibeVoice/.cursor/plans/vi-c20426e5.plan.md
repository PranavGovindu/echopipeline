<!-- c20426e5-1f77-4ef9-8471-ca814b1cecea 5290977e-5824-4dee-840f-51e010e46f7a -->
# VibeVoice Repository Overview

## Project Description

**VibeVoice** is Microsoft's open-source research framework for frontier voice AI, specifically designed for generating **expressive**, **long-form**, **multi-speaker** conversational audio such as podcasts from text. It addresses key challenges in traditional TTS systems including scalability, speaker consistency, and natural turn-taking.

## Key Features

### 1. Two Model Variants

- **Long-form multi-speaker model**: Synthesizes conversational/single-speaker speech up to **90 minutes** with up to **4 distinct speakers**
- **Realtime streaming TTS model** ([VibeVoice-Realtime-0.5B](docs/vibevoice-realtime-0.5b.md)): 
  - Produces initial audible speech in ~**300ms**
  - Supports **streaming text input**
  - Single-speaker real-time speech generation
  - Parameter size: 0.5B (deployment-friendly)
  - Based on Qwen2.5 0.5b

### 2. Core Innovation

- Uses continuous speech tokenizers (Acoustic and Semantic) operating at ultra-low frame rate of **7.5 Hz**
- Employs a **next-token diffusion** framework
- Leverages Large Language Model (LLM) to understand textual context and dialogue flow
- Uses diffusion head to generate high-fidelity acoustic details

## Technology Stack

### Dependencies ([pyproject.toml](pyproject.toml))

```
- Python >= 3.9
- PyTorch
- Transformers 4.51.3 (specific version required)
- Accelerate 1.6.0
- Diffusers
- FastAPI + Uvicorn (for web service)
- Gradio (for UI)
- Librosa, NumPy, SciPy (audio processing)
- aiortc, av (WebRTC/streaming)
```

## Repository Structure

### Core Package: `vibevoice/`

1. **[modular/](vibevoice/modular/)** - Model implementations

   - `modeling_vibevoice_streaming.py` - Main streaming model
   - `modeling_vibevoice_streaming_inference.py` - Inference-optimized version
   - `configuration_vibevoice_streaming.py` - Model configuration
   - `modular_vibevoice_diffusion_head.py` - Diffusion head for acoustic generation
   - `modular_vibevoice_tokenizer.py` - Speech tokenization
   - `streamer.py` - Audio streaming utilities

2. **[processor/](vibevoice/processor/)** - Audio processing

   - `vibevoice_streaming_processor.py` - Text-to-audio preprocessing
   - `vibevoice_tokenizer_processor.py` - Tokenization handling

3. **[schedule/](vibevoice/schedule/)** - Diffusion scheduling

   - `dpm_solver.py` - DPM solver for diffusion
   - `timestep_sampler.py` - Timestep sampling strategies

4. **[configs/](vibevoice/configs/)** - Model configurations

   - `qwen2.5_1.5b_64k.json` - 1.5B parameter model config
   - `qwen2.5_7b_32k.json` - 7B parameter model config

### Demo Applications: `demo/`

1. **[vibevoice_realtime_demo.py](demo/vibevoice_realtime_demo.py)** - WebSocket-based real-time demo

   - Launches FastAPI server on port 3000
   - Real-time streaming TTS service
   - WebRTC support

2. **[realtime_model_inference_from_file.py](demo/realtime_model_inference_from_file.py)** - File-based inference

   - Reads text from files
   - Generates audio output
   - Supports multiple voices (Carter, Davis, Emma, Frank, Grace, Mike, Samuel)
   - Configurable CFG scale, device selection (CUDA/MPS/CPU)

3. **[web/](demo/web/)** - Web interface

   - `app.py` - FastAPI application with WebSocket streaming
   - `index.html` - Frontend UI for real-time interaction
   - Handles audio streaming via WebSocket with PCM16 encoding

4. **[voices/streaming_model/](demo/voices/streaming_model/)** - Pre-trained voice embeddings

   - Multiple speaker voices (en-Carter, en-Davis, en-Emma, en-Frank, en-Grace, en-Mike, in-Samuel)
   - `.pt` files containing prefilled model outputs for voice conditioning

5. **[text_examples/](demo/text_examples/)** - Sample scripts for testing

## Key Capabilities

### Performance Benchmarks

- **LibriSpeech test-clean**: WER 2.00%, Speaker Similarity 0.695
- **SEED test-en**: WER 2.05%, Speaker Similarity 0.633
- Competitive with VALL-E 2, Voicebox, Seed-TTS, MaskGCT

### Supported Languages

- **English** (primary)
- **Chinese** (supported)
- Other languages may produce unexpected outputs

### Limitations

- English and Chinese only
- No background noise, music, or sound effects
- No overlapping speech in conversations
- Not recommended for commercial use without further testing
- Research and development purposes only

## How It Works

### Real-time Streaming Pipeline ([demo/web/app.py](demo/web/app.py))

1. **Text Input** → Processor tokenization
2. **Voice Selection** → Load prefilled prompt (speaker embedding)
3. **Model Generation** → Streaming diffusion-based audio generation

   - Uses AudioStreamer for chunk-by-chunk generation
   - Configurable inference steps (default: 5)
   - CFG scale for quality control (default: 1.5)

4. **Audio Output** → PCM16 encoding at 24kHz sample rate
5. **WebSocket Streaming** → Real-time audio delivery to client

### Device Support

- **CUDA** (recommended): bfloat16, flash_attention_2
- **MPS** (Apple Silicon): float32, SDPA
- **CPU**: float32, SDPA

## License & Responsible AI

- **License**: MIT License (Copyright 2025 Microsoft)
- **Important Notes**:
  - Potential for deepfakes and disinformation
  - Users must ensure transcripts are reliable
  - Disclose AI-generated content
  - Use responsibly and comply with applicable laws
  - Model inherits biases from base LLM (Qwen2.5)

## Getting Started

### Installation

```bash
git clone https://github.com/microsoft/VibeVoice.git
cd VibeVoice/
pip install -e .
```

### Usage Examples

**Real-time WebSocket Demo:**

```bash
python demo/vibevoice_realtime_demo.py --model_path microsoft/VibeVoice-Realtime-0.5B
```

**File-based Inference:**

```bash
python demo/realtime_model_inference_from_file.py \
  --model_path microsoft/VibeVoice-Realtime-0.5B \
  --txt_path demo/text_examples/1p_vibevoice.txt \
  --speaker_name Carter
```

## Resources

- **Hugging Face**: [microsoft/VibeVoice-Realtime-0.5B](https://huggingface.co/microsoft/VibeVoice-Realtime-0.5B)
- **Technical Report**: [arXiv:2508.19205](https://arxiv.org/pdf/2508.19205)
- **Project Page**: [microsoft.github.io/VibeVoice](https://microsoft.github.io/VibeVoice)
- **Colab Demo**: Available for quick testing

## Architecture Summary

The system uses:

- **LLM backbone** (Qwen2.5 0.5B/1.5B/7B) for understanding text context
- **Diffusion head** for acoustic token generation
- **Acoustic tokenizer** (7.5 Hz frame rate) for efficient audio representation
- **Streaming architecture** with windowed processing for real-time generation
- **DPM solver** for fast diffusion sampling (typically 5 steps)